# In-Context-learning-Papers    

![](https://img.shields.io/badge/PRs-welcome-brightgreen) ![](https://img.shields.io/github/stars/init-neok/In-Context-learning-Papers?style=social) 

Inspired by the [repository](https://github.com/SinclairCoder/Instruction-Tuning-Papers) about `instruction tuning`, we build this repository to collect papers about In-Context Learning.
* It's my frist time to collect as a repository, so I will try my best to maintain it. If you have any suggestions, please feel free to leave an issue.
* What is In-Context Learning? It is an emergent ability which enables language models to perform better just by reading just a few similar demonstrations. 

## Papers
* **Language Models are Few-Shot Learners** `NeurIPS 2020`
  
   *Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei* [[paper]](https://arxiv.org/abs/2005.14165) 2020.5

* **What Makes Good In-Context Examples for GPT-3?** 
  
   *Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu Chen* [[paper]](https://arxiv.org/abs/2101.06804) 2021.1

* **MetaICL: Learning to Learn In Context** `NAACL 2022`
  
   *[Sewon Min](https://shmsw25.github.io/), Mike Lewis, Luke Zettlemoyer, Hannaneh Hajishirzi* [[paper]](https://arxiv.org/abs/2110.15943) 2021.10

* **Multitask Prompted Training Enables Zero-Shot Task Generalization** `ICLR 2022`
  
   *Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, Alexander M. Rush* [[paper]](https://arxiv.org/abs/2110.08207) 2021.10

* **Meta-learning via Language Model In-context Tuning** `ACL 2022`
  
   *Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, He He* [[paper]](https://aclanthology.org/2022.acl-long.53/) 2021.10

* **A Survey on In-context Learning**
  
   *Qingxiu Dong, [Lei Li](https://lilei-nlp.github.io/), Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li and Zhifang Sui* [[paper]](https://arxiv.org/abs/2301.00234) 2022.12

* **Learning To Retrieve Prompts for In-Context Learning**
  
   *Ohad Rubin, Jonathan Herzig, Jonathan Berant* [[paper]](https://arxiv.org/abs/2112.08633) 2021.12

* **Calibrate Before Use: Improving Few-shot Performance of Language Models**
  
   *Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh* [[paper]](https://arxiv.org/abs/2102.09690) 2022.2

* **Active Example Selection for In-Context Learning**
  
   *Yiming Zhang, Shi Feng, Chenhao Tan* [[paper]](https://arxiv.org/abs/2211.04486) 2022.11

* **Self-Adaptive In-Context Learning: An Information Compression Perspective for In-Context Example Selection and Ordering**
  
   *Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, Lingpeng Kong* [[paper]](https://arxiv.org/abs/2212.10375) 2022.12

* **Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers** `ACL 2023`
  
   *Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, Furu Wei* [[paper]](https://arxiv.org/abs/2212.10559) 2022.12

* **Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?** `EMNLP 2022`
  
   *Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer* [[paper]](https://arxiv.org/abs/2202.12837) 2022.2

* **Compositional Exemplars for In-context Learning** `ICML 2023`
  
   *[Jiacheng Ye](https://jiacheng-ye.github.io/), Zhiyong Wu, Jiangtao Feng, Tao Yu, Lingpeng Kong* [[paper]](https://arxiv.org/abs/2302.05698) 2023.2

* **Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning** `EMNLP 2023 BEST PAPER AWARDüèÜ`
  
   Talk from author [[link]](https://event.baai.ac.cn/live/733)
   Zhihu [[link]](https://zhuanlan.zhihu.com/p/439876633)

   *Lean Wang, [Lei Li](https://lilei-nlp.github.io/), Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, Xu Sun* [[paper]](https://arxiv.org/abs/2305.14160) 2023.5

* **In-Context Learning Creates Task Vectors** `EMNLP 2023`
  
   *Roee Hendel, Mor Geva, Amir Globerson* [[paper]](https://aclanthology.org/2023.findings-emnlp.624/) 2023.10

* **In-Context Learning with Iterative Demonstration Selection**
  
   *Chengwei Qin, Aston Zhang, Anirudh Dagar, Wenming Ye* [[paper]](https://arxiv.org/abs/2310.09881) 2023.10

* **Exploring the Relationship between In-Context Learning and Instruction Tuning**
  
   *Hanyu Duan, Yixuan Tang, Yi Yang, Ahmed Abbasi, Kar Yan Tam* [[paper]](https://arxiv.org/abs/2311.10367) 2023.11

* **Adapt in Contexts: Retrieval-Augmented Domain Adaptation via In-Context Learning** `EMNLP 2023`
  
   *Quanyu Long, Wenya Wang, Sinno Pan* [[paper]](https://aclanthology.org/2023.emnlp-main.402/) 2023.11











## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=init-neok/In-Context-learning-Papers&type=Date)](https://star-history.com/#init-neok/In-Context-learning-Papers&Date)
